import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from matplotlib.patches import Circle, FancyBboxPatch
import plotly.graph_objects as go
import plotly.express as px
from sklearn.datasets import make_classification, make_circles
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from scipy.signal import convolve2d
import tensorflow as tf

# --- CONFIGURA√á√ÉO DA P√ÅGINA E CSS ---
st.set_page_config(
    page_title="Guia Avan√ßado de Redes Neurais",
    page_icon="üß†",
    layout="wide"
)

st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 2.2rem;
        color: #ff7f0e;
        border-bottom: 2px solid #ff7f0e;
        padding-bottom: 0.5rem;
        margin: 2rem 0 1rem 0;
    }
    .info-box {
        background-color: #f0f8ff;
        padding: 1rem;
        border-radius: 10px;
        border-left: 5px solid #1f77b4;
        margin: 1rem 0;
    }
    .formula-box {
        background-color: #fff8dc;
        padding: 1rem;
        border-radius: 10px;
        border: 2px solid #ffa500;
        margin: 1rem 0;
        text-align: center;
    }
    .code-box {
        border: 1px solid #ddd;
        border-radius: 5px;
        padding: 10px;
        background-color: #f9f9f9;
    }
</style>
""", unsafe_allow_html=True)

# --- T√çTULO PRINCIPAL ---
st.markdown('<h1 class="main-header">üß† Guia Avan√ßado de Redes Neurais</h1>', unsafe_allow_html=True)
st.markdown("<p style='text-align: center;'>Uma abordagem did√°tica e interativa para estudantes de P√≥s-Gradua√ß√£o.</p>", unsafe_allow_html=True)


# --- SIDEBAR DE NAVEGA√á√ÉO ---
st.sidebar.title("üìö Navega√ß√£o")
secao = st.sidebar.selectbox(
    "Escolha um t√≥pico:",
    [
        "üîç Introdu√ß√£o",
        "üîß O Perceptron",
        "üèõÔ∏è Arquitetura de Redes Neurais",
        "üß† Multilayer Perceptron (MLP) em A√ß√£o",
        "üîÑ Backpropagation",
        "üñºÔ∏è Redes Neurais Convolucionais (CNN)",
        "üìú Redes Neurais Recorrentes (RNN)",
        "üéÆ Playground Interativo"
    ]
)

# --- FUN√á√ïES AUXILIARES ---

# Fun√ß√£o para plotar o Perceptron (mantida do original)
def plot_perceptron(weights, bias, X, y, title="Perceptron"):
    fig, ax = plt.subplots(1, 2, figsize=(15, 6))
    
    # Gr√°fico 1: Arquitetura
    ax[0].set_xlim(0, 10); ax[0].set_ylim(0, 6)
    ax[0].set_title("Arquitetura do Perceptron", fontsize=14, fontweight='bold')
    for i in range(2):
        ax[0].add_patch(Circle((1, 2 + i * 2), 0.3, color='lightblue', ec='black'))
        ax[0].text(1, 2 + i * 2, f'x{i+1}', ha='center', va='center', fontweight='bold')
        ax[0].arrow(1.3, 2 + i * 2, 3.4, 1 - i * 2, head_width=0.1, fc='red', ec='red')
        ax[0].text(3, 2.5 + i * 0.5, f'w{i+1}={weights[i]:.2f}', fontsize=10, color='red')
    ax[0].add_patch(Circle((5.5, 3), 0.5, color='orange', ec='black'))
    ax[0].text(5.5, 3, 'Œ£', ha='center', va='center', fontsize=16, fontweight='bold')
    ax[0].text(5.5, 2.2, f'bias={bias:.2f}', ha='center', fontsize=10, color='blue')
    ax[0].arrow(6, 3, 2, 0, head_width=0.1, fc='green', ec='green')
    ax[0].add_patch(Circle((8.5, 3), 0.3, color='lightgreen', ec='black'))
    ax[0].text(8.5, 3, 'y', ha='center', va='center', fontweight='bold')
    ax[0].axis('off')

    # Gr√°fico 2: Dados e linha de decis√£o
    ax[1].scatter(X[y == 0, 0], X[y == 0, 1], c='red', marker='o', label='Classe 0')
    ax[1].scatter(X[y == 1, 0], X[y == 1, 1], c='blue', marker='s', label='Classe 1')
    if weights[1] != 0:
        x_line = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100)
        y_line = -(weights[0] * x_line + bias) / weights[1]
        ax[1].plot(x_line, y_line, 'k--', linewidth=2, label='Fronteira de Decis√£o')
    ax[1].set_xlabel('Feature 1'); ax[1].set_ylabel('Feature 2')
    ax[1].set_title('Classifica√ß√£o do Perceptron'); ax[1].legend(); ax[1].grid(True, alpha=0.3)
    
    return fig

# Fun√ß√£o para visualizar arquitetura de rede neural (mantida)
def plot_neural_network(layers):
    fig, ax = plt.subplots(figsize=(12, 8))
    layer_positions = np.linspace(1, 10, len(layers))
    max_neurons = max(layers)
    for i, (pos, neurons) in enumerate(zip(layer_positions, layers)):
        y_positions = np.linspace(1, max_neurons, neurons)
        for j, y_pos in enumerate(y_positions):
            ax.add_patch(Circle((pos, y_pos), 0.2, color='orange', ec='black'))
            if i < len(layers) - 1:
                next_y_positions = np.linspace(1, max_neurons, layers[i + 1])
                for next_y in next_y_positions:
                    ax.plot([pos + 0.2, layer_positions[i + 1] - 0.2], 
                           [y_pos, next_y], 'k-', alpha=0.3)
    labels = ['Entrada', 'Oculta', 'Sa√≠da']
    for i, pos in enumerate(layer_positions):
        if i < len(labels):
            ax.text(pos, max_neurons + 0.5, labels[i], ha='center', fontweight='bold')
    ax.set_xlim(0, 11); ax.set_ylim(0, max_neurons + 1); ax.axis('off')
    ax.set_title('Arquitetura da Rede Neural', fontsize=16, fontweight='bold')
    return fig

# Fun√ß√£o para plotar fronteira de decis√£o (NOVA)
def plot_decision_boundary(model, X, y):
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    fig = go.Figure()
    fig.add_trace(go.Contour(x=xx[0], y=yy[:, 0], z=Z, colorscale='RdBu', showscale=False, opacity=0.8))
    fig.add_trace(go.Scatter(x=X[y==0, 0], y=X[y==0, 1], mode='markers', marker_color='red', name='Classe 0'))
    fig.add_trace(go.Scatter(x=X[y==1, 0], y=X[y==1, 1], mode='markers', marker_color='blue', name='Classe 1'))
    fig.update_layout(title="Fronteira de Decis√£o da Rede Neural", xaxis_title="Feature 1", yaxis_title="Feature 2")
    return fig

# Implementa√ß√£o do Perceptron (mantida do original)
class SimplePerceptron:
    def __init__(self, learning_rate=0.01, epochs=100):
        self.lr, self.epochs = learning_rate, epochs
        self.weights, self.bias, self.errors = None, None, []
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features); self.bias = 0; self.errors = []
        for _ in range(self.epochs):
            errors = 0
            for xi, target in zip(X, y):
                update = self.lr * (target - self.predict(xi))
                self.weights += update * xi
                self.bias += update
                errors += int(update != 0.0)
            self.errors.append(errors)
            if errors == 0: break
    def predict(self, x):
        return 1 if np.dot(x, self.weights) + self.bias >= 0.0 else 0

# --- SE√á√ïES DO APLICATIVO ---

if secao == "üîç Introdu√ß√£o":
    # (C√≥digo da Introdu√ß√£o original, sem altera√ß√µes)
    st.markdown('<h2 class="section-header">O que s√£o Redes Neurais?</h2>', unsafe_allow_html=True)
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("""
        <div class="info-box">
        As <strong>Redes Neurais Artificiais (RNAs)</strong> s√£o modelos computacionais inspirados pela estrutura e funcionamento do c√©rebro humano. Elas constituem o n√∫cleo de muitos dos avan√ßos em Intelig√™ncia Artificial e s√£o capazes de aprender padr√µes complexos a partir de dados atrav√©s de um processo de treinamento.
        
        <h4>Componentes Fundamentais:</h4>
        <ul>
            <li><b>Neur√¥nios (ou N√≥s):</b> As unidades computacionais b√°sicas que recebem entradas, processam-nas e geram uma sa√≠da.</li>
            <li><b>Conex√µes e Pesos:</b> Cada conex√£o entre neur√¥nios possui um peso associado, que modula a for√ßa do sinal. O aprendizado ocorre pelo ajuste desses pesos.</li>
            <li><b>Bias:</b> Um par√¢metro extra, similar ao intercepto em uma regress√£o linear, que permite deslocar a fun√ß√£o de ativa√ß√£o.</li>
            <li><b>Fun√ß√£o de Ativa√ß√£o:</b> Determina a sa√≠da do neur√¥nio, introduzindo n√£o-linearidades que permitem √† rede aprender padr√µes complexos.</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)
    with col2:
        st.image("https://www.google.com/imgres?q=Artificial%20Neuron.png&imgurl=https%3A%2F%2Fimg.favpng.com%2F14%2F13%2F19%2Fartificial-neural-network-deep-learning-artificial-intelligence-machine-learning-neuron-png-favpng-R2h2Atu1ukdBmjhEyMzXmVtPD.jpg&imgrefurl=https%3A%2F%2Ffavpng.com%2Fpng_view%2Fbrain-artificial-neural-network-deep-learning-artificial-intelligence-machine-learning-neuron-png%2FkR6FGUUd&docid=xENqRVzuapGZeM&tbnid=kPEG4siK8nit4M&vet=12ahUKEwjqkseV2_iNAxUQq5UCHQNxG6YQM3oECHMQAA..i&w=820&h=440&hcb=2&ved=2ahUKEwjqkseV2_iNAxUQq5UCHQNxG6YQM3oECHMQAA", 
                 caption="Modelo de um neur√¥nio artificial.")
    
elif secao == "üîß O Perceptron":
    # (C√≥digo do Perceptron original, sem altera√ß√µes significativas)
    st.markdown('<h2 class="section-header">O Perceptron: O Bloco de Constru√ß√£o Fundamental</h2>', unsafe_allow_html=True)
    st.markdown("""O Perceptron, concebido por Frank Rosenblatt em 1957, √© o modelo mais simples de um neur√¥nio artificial. Ele serve como um classificador bin√°rio linear, o que significa que pode aprender a separar dados que s√£o linearmente separ√°veis.""")
    
    col1, col2 = st.columns([1, 1])
    with col1:
        st.markdown("""
        <div class="info-box">
        <h4>Funcionamento Matem√°tico</h4>
        1. <strong>Soma Ponderada (z):</strong> As entradas (x) s√£o multiplicadas pelos seus respectivos pesos (w) e somadas, junto com o bias (b).<br>
        2. <strong>Fun√ß√£o de Ativa√ß√£o:</strong> Uma fun√ß√£o degrau (Heaviside) √© aplicada √† soma ponderada. Se `z` for maior ou igual a um limiar (geralmente 0), a sa√≠da √© 1; caso contr√°rio, √© 0.
        </div>
        """, unsafe_allow_html=True)
        st.markdown("""
        <div class="formula-box">
        z = (‚àë w·µ¢x·µ¢) + b <br>
        y = { 1 se z ‚â• 0; 0 se z < 0 }
        </div>
        """, unsafe_allow_html=True)
        st.warning("**Limita√ß√£o Cr√≠tica:** O Perceptron s√≥ pode resolver problemas linearmente separ√°veis. Ele falha em problemas como o XOR.", icon="‚ö†Ô∏è")

    with col2:
        st.subheader("üß™ Demonstra√ß√£o de Treinamento")
        if st.button("üöÄ Treinar Perceptron em Dados Lineares"):
            X_train, y_train = make_classification(n_samples=100, n_features=2, n_redundant=0, 
                                                 n_informative=2, n_clusters_per_class=1, 
                                                 flip_y=0, random_state=1)
            perceptron = SimplePerceptron(learning_rate=0.1, epochs=100)
            perceptron.fit(X_train, y_train)
            st.session_state.perceptron_trained = perceptron
            st.session_state.perceptron_data = (X_train, y_train)

    if 'perceptron_trained' in st.session_state:
        perceptron = st.session_state.perceptron_trained
        X_train, y_train = st.session_state.perceptron_data
        
        fig_result = plot_perceptron(perceptron.weights, perceptron.bias, X_train, y_train, "Resultado do Treinamento")
        st.pyplot(fig_result)
        
        st.markdown("A linha tracejada representa a fronteira de decis√£o que o Perceptron aprendeu para separar as duas classes.")

elif secao == "üèõÔ∏è Arquitetura de Redes Neurais":
    # (C√≥digo da se√ß√£o de Redes Neurais, renomeada para focar na arquitetura)
    st.markdown('<h2 class="section-header">Arquitetura de Redes Neurais Multicamadas (MLP)</h2>', unsafe_allow_html=True)
    st.markdown("""
    Para superar a limita√ß√£o do Perceptron, combinamos m√∫ltiplos neur√¥nios em camadas. Uma **Rede Neural Multicamadas (MLP)** consiste em:
    - Uma **Camada de Entrada** que recebe os dados.
    - Uma ou mais **Camadas Ocultas** que realizam o processamento intermedi√°rio e a extra√ß√£o de caracter√≠sticas. √â aqui que a "m√°gica" acontece.
    - Uma **Camada de Sa√≠da** que produz o resultado final (e.g., uma probabilidade de classe).

    A presen√ßa de camadas ocultas e fun√ß√µes de ativa√ß√£o n√£o-lineares permite que os MLPs aprendam fronteiras de decis√£o complexas e n√£o-lineares.
    """)
    col1, col2 = st.columns([2, 1])
    with col1:
        st.subheader("üìä Visualiza√ß√£o da Arquitetura")
        layers = [
            st.session_state.get('input_neurons', 4), 
            st.session_state.get('hidden_neurons', 8), 
            st.session_state.get('hidden_neurons_2', 5), 
            st.session_state.get('output_neurons', 2)
        ]
        fig_nn = plot_neural_network(layers)
        st.pyplot(fig_nn)
    with col2:
        st.subheader("üéÆ Configurar Arquitetura")
        st.session_state.input_neurons = st.slider("Neur√¥nios de Entrada", 2, 10, 4)
        st.session_state.hidden_neurons = st.slider("Neur√¥nios na Camada Oculta 1", 2, 20, 8)
        st.session_state.hidden_neurons_2 = st.slider("Neur√¥nios na Camada Oculta 2", 2, 20, 5)
        st.session_state.output_neurons = st.slider("Neur√¥nios de Sa√≠da", 1, 5, 2)
    
    st.subheader("üìà Fun√ß√µes de Ativa√ß√£o N√£o-Lineares")
    st.markdown("Fun√ß√µes de ativa√ß√£o como Sigmoid, Tanh e, especialmente, **ReLU (Rectified Linear Unit)**, s√£o essenciais. A ReLU √© a mais popular atualmente devido √† sua simplicidade e efic√°cia em mitigar o problema do desaparecimento do gradiente.")
    x_act = np.linspace(-5, 5, 100)
    df_act = pd.DataFrame({
        'x': x_act,
        'ReLU': np.maximum(0, x_act),
        'Sigmoid': 1 / (1 + np.exp(-x_act)),
        'Tanh': np.tanh(x_act)
    })
    fig_act = px.line(df_act, x='x', y=['ReLU', 'Sigmoid', 'Tanh'], title="Fun√ß√µes de Ativa√ß√£o Comuns")
    st.plotly_chart(fig_act, use_container_width=True)

# --- NOVA SE√á√ÉO: MLP em A√ß√£o ---
elif secao == "üß† Multilayer Perceptron (MLP) em A√ß√£o":
    st.markdown('<h2 class="section-header">MLP em A√ß√£o: Resolvendo Problemas N√£o-Lineares</h2>', unsafe_allow_html=True)
    st.markdown("""
    Vamos ver o MLP resolver um problema que o Perceptron n√£o consegue: classificar dados que formam c√≠rculos conc√™ntricos.
    Usaremos a implementa√ß√£o `MLPClassifier` da biblioteca `scikit-learn` para demonstrar o poder das camadas ocultas.
    """)

    col1, col2 = st.columns([1, 2])
    with col1:
        st.subheader("‚öôÔ∏è Par√¢metros do Modelo")
        hidden_layers = st.slider("Neur√¥nios na Camada Oculta", 2, 64, 10, key='mlp_h')
        activation = st.selectbox("Fun√ß√£o de Ativa√ß√£o", ["relu", "tanh", "logistic"], key='mlp_a')
        learning_rate = st.slider("Taxa de Aprendizado", 0.001, 0.1, 0.01, format="%.3f", key='mlp_lr')
        epochs = st.slider("M√°ximo de √âpocas", 100, 1000, 300, key='mlp_e')
        
        train_button = st.button("üöÄ Treinar MLP", type="primary")

    # Gerar dados
    X, y = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=1)
    X_scaled = StandardScaler().fit_transform(X)

    with col2:
        st.subheader("üìä Dados e Resultado")
        if train_button:
            model = MLPClassifier(
                hidden_layer_sizes=(hidden_layers,),
                activation=activation,
                solver='adam',
                learning_rate_init=learning_rate,
                max_iter=epochs,
                random_state=1
            )
            model.fit(X_scaled, y)
            st.session_state.mlp_model = model
            
            # Plotar curva de perda
            loss_fig = px.line(y=model.loss_curve_, labels={'x': '√âpoca', 'y': 'Perda'}, title="Curva de Perda do Treinamento")
            st.plotly_chart(loss_fig, use_container_width=True)
        
        if 'mlp_model' in st.session_state:
            # Plotar fronteira de decis√£o
            boundary_fig = plot_decision_boundary(st.session_state.mlp_model, X_scaled, y)
            st.plotly_chart(boundary_fig, use_container_width=True)
            st.info(f"Acur√°cia do modelo: {st.session_state.mlp_model.score(X_scaled, y):.2%}")
        else:
            fig_data = px.scatter(x=X_scaled[:, 0], y=X_scaled[:, 1], color=y.astype(str), title="Dataset 'C√≠rculos' (N√£o-Linearmente Separ√°vel)")
            st.plotly_chart(fig_data, use_container_width=True)
            st.info("Clique em 'Treinar MLP' para ver o resultado.")

elif secao == "üîÑ Backpropagation":
    # (C√≥digo da se√ß√£o de Backpropagation, sem altera√ß√µes significativas)
    st.markdown('<h2 class="section-header">Backpropagation: O Algoritmo de Aprendizado</h2>', unsafe_allow_html=True)
    st.markdown("""
    O **Backpropagation** (retropropaga√ß√£o do erro) √© o algoritmo que permite que as redes neurais aprendam. Ele funciona em conjunto com um m√©todo de otimiza√ß√£o, como o **Gradiente Descendente**.
    A ideia central √© calcular o gradiente da fun√ß√£o de perda (erro) em rela√ß√£o a cada peso e bias da rede. Este gradiente aponta a dire√ß√£o de maior crescimento do erro, ent√£o ajustamos os pesos na dire√ß√£o *oposta* para minimiz√°-lo.
    """)
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("""
        <div class="info-box">
        <h4>Processo Simplificado:</h4>
        <ol>
            <li><b>Forward Pass:</b> Os dados de entrada s√£o passados pela rede para gerar uma predi√ß√£o.</li>
            <li><b>C√°lculo da Perda:</b> A predi√ß√£o √© comparada com o valor real para calcular o erro (e.g., erro quadr√°tico m√©dio).</li>
            <li><b>Backward Pass:</b> A "m√°gica" acontece aqui. Usando a <b>Regra da Cadeia</b> do c√°lculo, o erro √© propagado de volta, da camada de sa√≠da para a de entrada, calculando a contribui√ß√£o de cada peso para o erro total.</li>
            <li><b>Atualiza√ß√£o dos Pesos:</b> Os pesos s√£o atualizados na dire√ß√£o oposta ao seu gradiente, diminuindo o erro.</li>
        </ol>
        </div>
        """, unsafe_allow_html=True)
    with col2:
        st.markdown("""
        <div class="formula-box">
        <b>Regra da Cadeia (conceitual):</b><br>
        <sup>‚àÇErro</sup>‚ÅÑ<sub>‚àÇpeso</sub> = <sup>‚àÇErro</sup>‚ÅÑ<sub>‚àÇsa√≠da</sub> √ó <sup>‚àÇsa√≠da</sup>‚ÅÑ<sub>‚àÇsoma</sub> √ó <sup>‚àÇsoma</sup>‚ÅÑ<sub>‚àÇpeso</sub>
        <br><br>
        <b>Atualiza√ß√£o do Peso (Gradiente Descendente):</b><br>
        peso<sub>novo</sub> = peso<sub>antigo</sub> - Œ∑ √ó <sup>‚àÇErro</sup>‚ÅÑ<sub>‚àÇpeso</sub>
        <br><i>(Œ∑ √© a taxa de aprendizado)</i>
        </div>
        """, unsafe_allow_html=True)

# --- NOVA SE√á√ÉO: CNN ---
elif secao == "üñºÔ∏è Redes Neurais Convolucionais (CNN)":
    st.markdown('<h2 class="section-header">Redes Neurais Convolucionais (CNNs)</h2>', unsafe_allow_html=True)
    st.markdown("""
    CNNs s√£o uma classe de redes neurais especializada no processamento de dados com uma topologia de grade, como imagens. Elas utilizam duas opera√ß√µes fundamentais: **convolu√ß√£o** e **pooling** para extrair hierarquias de caracter√≠sticas espaciais.
    """)
    
    with st.expander("üëÅÔ∏è A Opera√ß√£o de Convolu√ß√£o", expanded=True):
        st.markdown("""
        A convolu√ß√£o aplica um **filtro (ou kernel)** sobre a imagem de entrada. O filtro √© uma pequena matriz de pesos que desliza sobre a imagem, computando o produto de ponto em cada posi√ß√£o. O resultado √© um **mapa de caracter√≠sticas (feature map)**, que destaca padr√µes espec√≠ficos, como bordas, texturas ou formas.
        """)
        
        col1, col2 = st.columns([1, 2])
        with col1:
            st.subheader("Filtros Interativos")
            image = np.array([
                [0, 0, 0, 10, 10, 10, 0, 0, 0],
                [0, 0, 0, 10, 10, 10, 0, 0, 0],
                [0, 0, 0, 10, 10, 10, 0, 0, 0],
                [0, 0, 0, 10, 10, 10, 0, 0, 0],
                [0, 0, 0, 10, 0, 10, 0, 0, 0],
                [0, 0, 0, 10, 0, 10, 0, 0, 0],
                [0, 0, 0, 10, 0, 10, 0, 0, 0],
            ])

            kernels = {
                "Detector de Borda Vertical": np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]),
                "Detector de Borda Horizontal": np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]]),
                "Sharpen (Agu√ßar)": np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
            }
            kernel_choice = st.selectbox("Escolha um Kernel:", list(kernels.keys()))
            kernel = kernels[kernel_choice]
            
        with col2:
            convolved_image = convolve2d(image, kernel, mode='valid')
            fig, ax = plt.subplots(1, 3, figsize=(12, 4))
            ax[0].imshow(image, cmap='gray'); ax[0].set_title('Imagem de Entrada')
            ax[1].imshow(kernel, cmap='gray'); ax[1].set_title('Kernel')
            ax[2].imshow(convolved_image, cmap='gray'); ax[2].set_title('Mapa de Caracter√≠sticas')
            for a in ax: a.axis('off')
            st.pyplot(fig)

    with st.expander("üìâ A Opera√ß√£o de Pooling", expanded=False):
        st.markdown("""
        O pooling (geralmente **Max Pooling**) reduz a dimensionalidade espacial dos mapas de caracter√≠sticas. Ele opera em uma janela (e.g., 2x2) e seleciona o valor m√°ximo, tornando a representa√ß√£o mais robusta a pequenas transla√ß√µes e reduzindo a carga computacional.
        """)
        input_map = np.array([[1, 4, 2, 8], [9, 3, 5, 7], [2, 6, 1, 0], [8, 4, 3, 5]])
        output_map = np.array([
            [np.max(input_map[0:2, 0:2]), np.max(input_map[0:2, 2:4])],
            [np.max(input_map[2:4, 0:2]), np.max(input_map[2:4, 2:4])]
        ])
        
        fig, ax = plt.subplots(1, 2, figsize=(8, 4))
        ax[0].imshow(input_map, cmap='viridis'); ax[0].set_title("Antes do Pooling (4x4)")
        ax[1].imshow(output_map, cmap='viridis'); ax[1].set_title("Depois do Max Pooling 2x2 (2x2)")
        for a in ax:
            for i in range(a.get_images()[0].get_array().shape[0]):
                for j in range(a.get_images()[0].get_array().shape[1]):
                    a.text(j, i, a.get_images()[0].get_array()[i, j], ha="center", va="center", color="w")
        st.pyplot(fig)

    st.subheader("üèóÔ∏è Arquitetura T√≠pica de uma CNN")
    st.markdown("Uma CNN empilha camadas de `Convolu√ß√£o -> Ativa√ß√£o (ReLU) -> Pooling` m√∫ltiplas vezes, seguidas por camadas totalmente conectadas (como um MLP) para a classifica√ß√£o final.")
    st.image("https://miro.medium.com/v2/resize:fit:1400/1*uAeANQIOuSKOLFiJIhNB3A.png", caption="Exemplo de arquitetura de CNN para classifica√ß√£o de imagens (LeNet-5).")
    st.code("""
# Exemplo de c√≥digo de uma CNN simples com TensorFlow/Keras
model = tf.keras.models.Sequential([
    # Camada convolucional
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),
    # Camada de pooling
    tf.keras.layers.MaxPooling2D(2, 2),
    
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    
    # Achatamento para a camada densa
    tf.keras.layers.Flatten(),
    
    # Camada densa (MLP)
    tf.keras.layers.Dense(128, activation='relu'),
    # Camada de sa√≠da
    tf.keras.layers.Dense(10, activation='softmax')
])
""", language='python')

# --- NOVA SE√á√ÉO: RNN ---
elif secao == "üìú Redes Neurais Recorrentes (RNN)":
    st.markdown('<h2 class="section-header">Redes Neurais Recorrentes (RNNs)</h2>', unsafe_allow_html=True)
    st.markdown("""
    RNNs s√£o projetadas para trabalhar com dados sequenciais, como s√©ries temporais, texto ou √°udio. Sua caracter√≠stica definidora √© a **conex√£o recorrente**: a sa√≠da de um neur√¥nio em um passo de tempo √© alimentada de volta para si mesmo no pr√≥ximo passo de tempo. Isso cria um "estado oculto" que atua como uma **mem√≥ria**, permitindo que a rede retenha informa√ß√µes sobre o passado.
    """)
    
    st.subheader("üîÑ Visualizando a Recorr√™ncia")
    st.markdown("Uma RNN pode ser 'desdobrada' no tempo, revelando como a informa√ß√£o flui de um passo para o outro.")
    st.image("https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png", caption="Uma RNN 'enrolada' (esquerda) e 'desdobrada' no tempo (direita). Fonte: Chris Olah's Blog.")

    st.warning("‚ö†Ô∏è **Problema do Gradiente:** RNNs simples sofrem com o desaparecimento ou explos√£o do gradiente em sequ√™ncias longas, dificultando o aprendizado de depend√™ncias de longo prazo.")
    
    with st.expander("üß† Solu√ß√£o: LSTM e GRU", expanded=True):
        st.markdown("""
        **Long Short-Term Memory (LSTM)** e **Gated Recurrent Unit (GRU)** s√£o tipos avan√ßados de RNNs projetados para resolver esse problema. Elas usam "port√µes" (gates) ‚Äî mecanismos de redes neurais que regulam o fluxo de informa√ß√£o.
        - **Forget Gate:** Decide qual informa√ß√£o do estado anterior deve ser descartada.
        - **Input Gate:** Decide qual nova informa√ß√£o deve ser armazenada.
        - **Output Gate:** Decide o que ser√° produzido como sa√≠da.
        
        Esses port√µes permitem que a rede aprenda a reter informa√ß√µes relevantes por longos per√≠odos e a descartar o que n√£o √© importante.
        """)
        st.image("https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png", caption="A estrutura de uma c√©lula LSTM com seus port√µes. Fonte: Chris Olah's Blog.")
    
    st.subheader("üèóÔ∏è Exemplo de Arquitetura para An√°lise de Sentimentos")
    st.markdown("Uma arquitetura comum para classifica√ß√£o de texto envolve uma camada de Embedding (para converter palavras em vetores), seguida por uma camada LSTM e, finalmente, uma camada densa para a sa√≠da.")
    st.code("""
# Exemplo de c√≥digo de um modelo LSTM para classifica√ß√£o de texto
model = tf.keras.models.Sequential([
    # Converte palavras (inteiros) em vetores densos
    tf.keras.layers.Embedding(input_dim=10000, output_dim=64),
    
    # A camada LSTM que processa a sequ√™ncia de vetores
    tf.keras.layers.LSTM(64),
    
    # Camada densa para classifica√ß√£o
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid') # Sa√≠da bin√°ria (positivo/negativo)
])
""", language='python')

# --- SE√á√ÉO: PLAYGROUND INTERATIVO (MELHORADO) ---
elif secao == "üéÆ Playground Interativo":
    st.markdown('<h2 class="section-header">Playground Interativo de MLP</h2>', unsafe_allow_html=True)
    st.markdown("""
    <div class="info-box">
    Experimente diferentes configura√ß√µes e veja como elas afetam o desempenho e a fronteira de decis√£o de uma rede neural em tempo real!
    </div>
    """, unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns([1, 1, 2])
    
    with col1:
        st.subheader("üéØ Dados")
        dataset_type = st.selectbox("Tipo de Dataset", ["C√≠rculos", "Luas", "Linearmente Separ√°vel"])
        n_samples = st.slider("Amostras", 100, 1000, 300)
    
    with col2:
        st.subheader("üèóÔ∏è Arquitetura")
        hl_1 = st.slider("Neur√¥nios Camada 1", 1, 50, 10)
        hl_2 = st.slider("Neur√¥nios Camada 2", 0, 50, 5) # 0 para desativar
        activation_pg = st.selectbox("Ativa√ß√£o", ["relu", "tanh"], key='pg_act')
        
    # Gerar dados
    if dataset_type == "Linearmente Separ√°vel":
        X, y = make_classification(n_samples=n_samples, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, random_state=42)
    elif dataset_type == "C√≠rculos":
        X, y = make_circles(n_samples=n_samples, noise=0.1, factor=0.5, random_state=42)
    else: # Luas
        from sklearn.datasets import make_moons
        X, y = make_moons(n_samples=n_samples, noise=0.15, random_state=42)
    
    X_scaled = StandardScaler().fit_transform(X)
    
    with col3:
        st.subheader("üìä Dados de Entrada")
        fig_data = px.scatter(x=X_scaled[:, 0], y=X_scaled[:, 1], color=y.astype(str))
        fig_data.update_layout(showlegend=False, title=f"Dataset: {dataset_type}")
        st.plotly_chart(fig_data, use_container_width=True)

    st.markdown("---")
    st.subheader("‚öôÔ∏è Treinamento e Resultados")
    
    if st.button("üöÄ TREINAR REDE NEURAL", type="primary"):
        hidden_layers_pg = (hl_1,) if hl_2 == 0 else (hl_1, hl_2)
        
        model_pg = MLPClassifier(
            hidden_layer_sizes=hidden_layers_pg,
            activation=activation_pg,
            solver='adam',
            max_iter=500,
            random_state=1,
            early_stopping=True,
            n_iter_no_change=20
        )
        
        with st.spinner("Treinando o modelo... Isso pode levar alguns segundos."):
            model_pg.fit(X_scaled, y)
        
        st.session_state.pg_model = model_pg
        st.success("‚úÖ Treinamento Conclu√≠do!")

    if 'pg_model' in st.session_state:
        res_col1, res_col2 = st.columns(2)
        model = st.session_state.pg_model
        
        with res_col1:
            st.markdown("#### Fronteira de Decis√£o")
            boundary_fig = plot_decision_boundary(model, X_scaled, y)
            st.plotly_chart(boundary_fig, use_container_width=True)

        with res_col2:
            st.markdown("#### M√©tricas do Treinamento")
            accuracy = model.score(X_scaled, y)
            st.metric("Acur√°cia no Treino", f"{accuracy:.2%}")
            st.metric("N√∫mero de Itera√ß√µes", f"{model.n_iter_}")
            st.metric("Perda Final", f"{model.loss_:.4f}")
            
            loss_fig = px.line(y=model.loss_curve_, labels={'x': '√âpoca', 'y': 'Perda'}, title="Curva de Perda")
            st.plotly_chart(loss_fig, use_container_width=True)

# --- RODAP√â ---
st.markdown("---")
st.markdown("<div style='text-align: center; color: #666;'><strong>Guia Avan√ßado de Redes Neurais</strong> | Desenvolvido para o aprendizado aprofundado de conceitos de Machine Learning.</div>", unsafe_allow_html=True)