import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from matplotlib.patches import Circle
import plotly.graph_objects as go
import plotly.express as px
from sklearn.datasets import make_classification, make_circles, make_moons
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from scipy.signal import convolve2d
from sklearn.model_selection import train_test_split

# Bloco try-except para o TensorFlow, para n√£o impedir a execu√ß√£o se n√£o estiver instalado
try:
    import tensorflow as tf
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False


# --- CONFIGURA√á√ÉO DA P√ÅGINA E CSS ---
st.set_page_config(
    page_title="Guia Avan√ßado de Redes Neurais",
    page_icon="üß†",
    layout="wide"
)

st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 2.2rem;
        color: #ff7f0e;
        border-bottom: 2px solid #ff7f0e;
        padding-bottom: 0.5rem;
        margin: 2rem 0 1rem 0;
    }
    .info-box {
        background-color: #f0f8ff;
        padding: 1rem;
        border-radius: 10px;
        border-left: 5px solid #1f77b4;
        margin: 1rem 0;
    }
    .formula-box {
        background-color: #fff8dc;
        padding: 1rem;
        border-radius: 10px;
        border: 2px solid #ffa500;
        margin: 1rem 0;
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)


# --- T√çTULO PRINCIPAL ---
st.markdown('<h1 class="main-header">üß† Guia Did√°tico Interativo de Redes Neurais</h1>', unsafe_allow_html=True)


# --- SIDEBAR DE NAVEGA√á√ÉO (CONSOLIDADA) ---
st.sidebar.title("üìö Navega√ß√£o")
secao = st.sidebar.radio(
    "Escolha uma se√ß√£o:",
    [
        "üîç Introdu√ß√£o",
        "üîß Perceptron",
        "üåê Redes Neurais",
        "üîÑ Backpropagation",
        "üß† MLP em A√ß√£o",
        "üñºÔ∏è Redes Neurais Convolucionais (CNN)",
        "üìú Redes Neurais Recorrentes (RNN)",
        "üéÆ Playground Interativo"
    ]
)

# --- FUN√á√ïES AUXILIARES ---

# Fun√ß√£o para criar visualiza√ß√£o do perceptron (da sua vers√£o original)
def plot_perceptron(weights, bias, X, y, title="Perceptron"):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Gr√°fico 1: Arquitetura do Perceptron
    ax1.set_xlim(0, 10); ax1.set_ylim(0, 6)
    ax1.set_title("Arquitetura do Perceptron", fontsize=14, fontweight='bold')
    for i in range(2):
        ax1.add_patch(Circle((1, 2 + i * 2), 0.3, color='lightblue', ec='black'))
        ax1.text(1, 2 + i * 2, f'x{i+1}', ha='center', va='center', fontweight='bold')
        ax1.arrow(1.3, 2 + i * 2, 3.4, 1 - i * 2, head_width=0.1, head_length=0.1, fc='red', ec='red')
        ax1.text(3, 2.5 + i * 0.5, f'w{i+1}={weights[i]:.2f}', fontsize=10, color='red')
    ax1.add_patch(Circle((5.5, 3), 0.5, color='orange', ec='black'))
    ax1.text(5.5, 3, 'Œ£', ha='center', va='center', fontsize=16, fontweight='bold')
    ax1.text(5.5, 2.2, f'bias={bias:.2f}', ha='center', fontsize=10, color='blue')
    ax1.arrow(6, 3, 2, 0, head_width=0.1, head_length=0.1, fc='green', ec='green')
    ax1.add_patch(Circle((8.5, 3), 0.3, color='lightgreen', ec='black'))
    ax1.text(8.5, 3, 'y', ha='center', va='center', fontweight='bold')
    ax1.set_aspect('equal'); ax1.axis('off')
    
    # Gr√°fico 2: Dados e linha de decis√£o
    ax2.scatter(X[y == 0, 0], X[y == 0, 1], c='red', marker='o', label='Classe 0', s=50)
    ax2.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', marker='s', label='Classe 1', s=50)
    if weights[1] != 0:
        x_line = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100)
        y_line = -(weights[0] * x_line + bias) / weights[1]
        ax2.plot(x_line, y_line, 'k--', linewidth=2, label='Linha de Decis√£o')
    ax2.set_xlabel('Feature 1'); ax2.set_ylabel('Feature 2')
    ax2.set_title('Classifica√ß√£o do Perceptron'); ax2.legend(); ax2.grid(True, alpha=0.3)
    
    return fig

# Fun√ß√£o para visualizar rede neural (da sua vers√£o original)
def plot_neural_network(layers):
    fig, ax = plt.subplots(figsize=(12, 8))
    layer_positions = np.linspace(1, 10, len(layers))
    max_neurons = max(layers) if layers else 1
    colors = ['lightblue', 'orange', 'lightgreen']
    for i, (pos, neurons) in enumerate(zip(layer_positions, layers)):
        y_positions = np.linspace(1, max_neurons, neurons)
        color = colors[i % len(colors)]
        for j, y_pos in enumerate(y_positions):
            ax.add_patch(Circle((pos, y_pos), 0.2, color=color, ec='black'))
            if i < len(layers) - 1:
                next_y_positions = np.linspace(1, max_neurons, layers[i + 1])
                for next_y in next_y_positions:
                    ax.plot([pos + 0.2, layer_positions[i + 1] - 0.2], [y_pos, next_y], 'k-', alpha=0.3, linewidth=0.5)
    labels = ['Entrada', 'Oculta', 'Sa√≠da']
    for i, pos in enumerate(layer_positions):
        if i < len(labels):
            ax.text(pos, max_neurons + 0.5, labels[i], ha='center', fontweight='bold')
    ax.set_xlim(0, 11); ax.set_ylim(0, max_neurons + 1); ax.set_aspect('equal'); ax.axis('off')
    ax.set_title('Arquitetura da Rede Neural', fontsize=16, fontweight='bold')
    return fig

# Implementa√ß√£o simples do perceptron (da sua vers√£o original)
class SimplePerceptron:
    def __init__(self, learning_rate=0.01, epochs=100):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.weights = None
        self.bias = None
        self.errors = []
    
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.random.normal(0, 0.01, n_features)
        self.bias = 0
        self.errors = []
        for epoch in range(self.epochs):
            epoch_errors = 0
            for i in range(n_samples):
                prediction = self.predict(X[i])
                error = y[i] - prediction
                if error != 0:
                    self.weights += self.learning_rate * error * X[i]
                    self.bias += self.learning_rate * error
                    epoch_errors += 1
            self.errors.append(epoch_errors)
            if epoch_errors == 0: break
    
    def predict(self, x):
        activation = np.dot(x, self.weights) + self.bias
        return 1 if activation >= 0 else 0

# Fun√ß√£o para plotar fronteira de decis√£o (das vers√µes mais novas)
def plot_decision_boundary(model, X, y):
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    fig = go.Figure()
    fig.add_trace(go.Contour(x=xx[0], y=yy[:, 0], z=Z, colorscale='RdBu', showscale=False, opacity=0.8))
    fig.add_trace(go.Scatter(x=X[y==0, 0], y=X[y==0, 1], mode='markers', marker_color='red', name='Classe 0'))
    fig.add_trace(go.Scatter(x=X[y==1, 0], y=X[y==1, 1], mode='markers', marker_color='blue', name='Classe 1'))
    fig.update_layout(title="Fronteira de Decis√£o da Rede Neural", xaxis_title="Feature 1", yaxis_title="Feature 2")
    return fig

# --- SE√á√ïES DO APLICATIVO ---

if secao == "üîç Introdu√ß√£o":
    st.markdown('<h2 class="section-header">O que s√£o Redes Neurais?</h2>', unsafe_allow_html=True)
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("""
        <div class="info-box">
        As <strong>Redes Neurais Artificiais (RNAs)</strong> s√£o modelos computacionais inspirados pela estrutura e funcionamento do c√©rebro humano. Elas constituem o n√∫cleo de muitos dos avan√ßos em Intelig√™ncia Artificial e s√£o capazes de aprender padr√µes complexos a partir de dados atrav√©s de um processo de treinamento.
        
        <h4>Componentes Fundamentais:</h4>
        <ul>
            <li><b>Neur√¥nios (ou N√≥s):</b> As unidades computacionais b√°sicas que recebem entradas, processam-nas e geram uma sa√≠da.</li>
            <li><b>Conex√µes e Pesos:</b> Cada conex√£o entre neur√¥nios possui um peso associado, que modula a for√ßa do sinal. O aprendizado ocorre pelo ajuste desses pesos.</li>
            <li><b>Bias:</b> Um par√¢metro extra, similar ao intercepto em uma regress√£o linear, que permite deslocar a fun√ß√£o de ativa√ß√£o.</li>
            <li><b>Fun√ß√£o de Ativa√ß√£o:</b> Determina a sa√≠da do neur√¥nio, introduzindo n√£o-linearidades que permitem √† rede aprender padr√µes complexos.</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)
    with col2:
        st.image("images.png", caption="Modelo de um neur√¥nio artificial.")

# --- SE√á√ÉO PERCEPTRON (SUA VERS√ÉO ORIGINAL INTERATIVA) ---
elif secao == "üîß Perceptron":
    st.markdown('<h2 class="section-header">Perceptron - O Primeiro Neur√¥nio Artificial</h2>', unsafe_allow_html=True)
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown("""
        <div class="info-box">
        O <strong>Perceptron</strong> √© o modelo mais simples de neur√¥nio artificial, criado por Frank Rosenblatt em 1957.
        √â um classificador bin√°rio linear que pode separar dados linearmente separ√°veis.
        </div>
        """, unsafe_allow_html=True)
        st.markdown("### üìê Funcionamento Matem√°tico")
        st.markdown("""
        <div class="formula-box">
        <strong>Soma Ponderada:</strong><br>
        z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + bias<br><br>
        <strong>Fun√ß√£o de Ativa√ß√£o (Degrau):</strong><br>
        y = 1 se z ‚â• 0, caso contr√°rio y = 0
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.subheader("üéÆ Controles Interativos")
        w1 = st.slider("Peso w‚ÇÅ", -2.0, 2.0, 0.5, 0.1)
        w2 = st.slider("Peso w‚ÇÇ", -2.0, 2.0, -0.3, 0.1)
        bias = st.slider("Bias", -2.0, 2.0, 0.0, 0.1)
        
        np.random.seed(42)
        X_example, y_example = make_classification(n_samples=50, n_features=2, n_redundant=0, 
                                                 n_informative=2, n_clusters_per_class=1, 
                                                 random_state=1)
    
    st.subheader("üìä Visualiza√ß√£o do Perceptron (Manipula√ß√£o Manual)")
    st.markdown("Use os sliders acima para ver como os pesos e o bias afetam a arquitetura e a linha de decis√£o.")
    fig = plot_perceptron([w1, w2], bias, X_example, y_example)
    st.pyplot(fig)
    
    st.markdown("---")
    st.subheader("üß™ Treinamento do Perceptron")
    
    col1_train, col2_train = st.columns([1, 1])
    
    with col1_train:
        st.markdown("**Configura√ß√µes de Treinamento:**")
        learning_rate = st.slider("Taxa de Aprendizado", 0.01, 1.0, 0.1, 0.01, key='lr_p')
        epochs = st.slider("N√∫mero de √âpocas", 10, 200, 50, 10, key='ep_p')
        
        if st.button("üöÄ Treinar Perceptron"):
            X_train, y_train = make_classification(n_samples=100, n_features=2, n_redundant=0, 
                                                 n_informative=2, n_clusters_per_class=1, 
                                                 random_state=42)
            perceptron = SimplePerceptron(learning_rate=learning_rate, epochs=epochs)
            perceptron.fit(X_train, y_train)
            st.session_state.perceptron = perceptron
            st.session_state.X_train_p = X_train
            st.session_state.y_train_p = y_train
    
    with col2_train:
        if 'perceptron' in st.session_state:
            st.markdown("**Resultados do Treinamento:**")
            perceptron = st.session_state.perceptron
            X_train = st.session_state.X_train_p
            y_train = st.session_state.y_train_p
            
            fig_conv, ax = plt.subplots(figsize=(8, 4))
            ax.plot(range(1, len(perceptron.errors) + 1), perceptron.errors, marker='o')
            ax.set_xlabel('√âpoca'); ax.set_ylabel('N√∫mero de Erros')
            ax.set_title('Converg√™ncia do Perceptron'); ax.grid(True, alpha=0.3)
            st.pyplot(fig_conv)
            
            fig_result = plot_perceptron(perceptron.weights, perceptron.bias, X_train, y_train, "Resultado Final")
            st.pyplot(fig_result)

# --- SE√á√ÉO REDES NEURAIS (SUA VERS√ÉO ORIGINAL INTERATIVA) ---
elif secao == "üåê Redes Neurais":
    st.markdown('<h2 class="section-header">Redes Neurais Multicamadas (MLP)</h2>', unsafe_allow_html=True)
    
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("""
        <div class="info-box">
        As <strong>Redes Neurais Multicamadas</strong> (MLPs - Multi-Layer Perceptrons) superam as limita√ß√µes do perceptron simples,
        sendo capazes de resolver problemas n√£o-linearmente separ√°veis atrav√©s de camadas ocultas e fun√ß√µes de ativa√ß√£o n√£o-lineares.
        </div>
        """, unsafe_allow_html=True)
        st.markdown("""
        ### üèóÔ∏è Arquitetura
        - **Camada de Entrada**: Recebe os dados brutos.
        - **Camadas Ocultas**: Processam e extraem caracter√≠sticas dos dados.
        - **Camada de Sa√≠da**: Produz o resultado final.
        """)
    
    with col2:
        st.subheader("üéÆ Configurar Arquitetura")
        input_neurons = st.number_input("Neur√¥nios de Entrada", 2, 10, 3, key='nn_in')
        hidden_neurons = st.number_input("Neur√¥nios Ocultos", 2, 20, 5, key='nn_hid')
        output_neurons = st.number_input("Neur√¥nios de Sa√≠da", 1, 5, 1, key='nn_out')
        layers = [input_neurons, hidden_neurons, output_neurons]

    st.subheader("üìä Visualiza√ß√£o da Arquitetura")
    st.markdown("Use os controles acima para montar a arquitetura da sua rede.")
    fig_nn = plot_neural_network(layers)
    st.pyplot(fig_nn)
    
    st.markdown("---")
    st.subheader("üìà Fun√ß√µes de Ativa√ß√£o Comuns")
    st.markdown("Essas fun√ß√µes introduzem n√£o-linearidade, permitindo que a rede aprenda padr√µes complexos.")
    x = np.linspace(-5, 5, 100)
    sigmoid = 1 / (1 + np.exp(-x))
    relu = np.maximum(0, x)
    tanh = np.tanh(x)
    
    fig_activation, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
    fig_activation.suptitle("Visualiza√ß√£o de Fun√ß√µes de Ativa√ß√£o", fontsize=16)
    ax1.plot(x, sigmoid, 'b-', linewidth=2); ax1.set_title('Sigmoid'); ax1.grid(True, alpha=0.3); ax1.set_ylim(-0.1, 1.1)
    ax2.plot(x, relu, 'r-', linewidth=2); ax2.set_title('ReLU'); ax2.grid(True, alpha=0.3)
    ax3.plot(x, tanh, 'g-', linewidth=2); ax3.set_title('Tanh'); ax3.grid(True, alpha=0.3); ax3.set_ylim(-1.1, 1.1)
    ax4.plot(x, sigmoid, 'b-', label='Sigmoid'); ax4.plot(x, relu, 'r-', label='ReLU'); ax4.plot(x, tanh, 'g-', label='Tanh')
    ax4.set_title('Compara√ß√£o'); ax4.legend(); ax4.grid(True, alpha=0.3)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    st.pyplot(fig_activation)

elif secao == "üîÑ Backpropagation":
    st.markdown('<h2 class="section-header">Backpropagation: Um Passo de Cada Vez</h2>', unsafe_allow_html=True)
    st.markdown("""
    O Backpropagation pode parecer uma "caixa preta". Vamos abri-la e executar um √∫nico passo de treinamento de forma interativa. 
    Veremos exatamente como a rede usa o erro para descobrir como ajustar seus pesos.
    
    Nosso cen√°rio: uma rede com **1 neur√¥nio**, **2 entradas**, e fun√ß√£o de ativa√ß√£o **Sigmoid**.
    """)

    # --- Configura√ß√£o do Exemplo Interativo ---
    col1, col2 = st.columns([1, 1.2])

    with col1:
        st.subheader("‚öôÔ∏è Controles")
        learning_rate = st.slider("Taxa de Aprendizado (Œ∑)", 0.01, 1.0, 0.5, 0.01)

        # Inicializa o estado da sess√£o para controlar os passos
        if 'bp_step' not in st.session_state:
            st.session_state.bp_step = 0

        def next_step():
            st.session_state.bp_step += 1
        
        def reset_steps():
            st.session_state.bp_step = 0
            # Reinicia os pesos para consist√™ncia
            st.session_state.w1 = 0.3
            st.session_state.w2 = -0.5
            st.session_state.bias = 0.1

        # Inicializa os pesos se n√£o existirem
        if 'w1' not in st.session_state:
            reset_steps()

        c1, c2 = st.columns(2)
        c1.button("Pr√≥ximo Passo ‚û°Ô∏è", on_click=next_step, type="primary", use_container_width=True)
        c2.button("Reiniciar üîÑ", on_click=reset_steps, use_container_width=True)

        # --- Par√¢metros do nosso exemplo ---
        x = np.array([2.0, 3.0])
        y_real = 1.0
        w = np.array([st.session_state.w1, st.session_state.w2])
        bias = st.session_state.bias

    with col2:
        st.subheader("Visualiza√ß√£o da Descida do Gradiente")
        # Gr√°fico simples para ilustrar a descida
        w_space = np.linspace(-1, 1, 100)
        # Fun√ß√£o de erro simplificada (par√°bola) para fins de visualiza√ß√£o
        error_space = (w_space - 0.7)**2 
        
        fig, ax = plt.subplots()
        ax.plot(w_space, error_space, label="Superf√≠cie de Erro")
        ax.set_xlabel("Valor do Peso (Ex: w1)")
        ax.set_ylabel("Erro")
        ax.set_title("O Objetivo: Atingir o M√≠nimo do Erro")
        
        # Ponto inicial
        ax.plot(st.session_state.w1, (st.session_state.w1 - 0.7)**2, 'ro', markersize=10, label="Peso Atual")

        if st.session_state.bp_step >= 5:
            w_novo_calculado = w[0] - learning_rate * st.session_state.grad_w1
            ax.plot(w_novo_calculado, (w_novo_calculado - 0.7)**2, 'go', markersize=10, label="Peso Novo")
            ax.annotate("", xy=(w_novo_calculado, (w_novo_calculado-0.7)**2), xytext=(st.session_state.w1, (st.session_state.w1-0.7)**2),
                        arrowprops=dict(arrowstyle="->", color="purple", lw=2))
        
        ax.legend()
        st.pyplot(fig)


    st.markdown("---")
    st.subheader("üîç O Processo Detalhado")

    # Passo 0: Estado Inicial
    st.markdown("##### üèÅ Estado Inicial")
    st.markdown(f"""
    - **Entradas:** `x‚ÇÅ = {x[0]}`, `x‚ÇÇ = {x[1]}`
    - **Alvo Real:** `y_real = {y_real}`
    - **Pesos Iniciais:** `w‚ÇÅ = {w[0]:.2f}`, `w‚ÇÇ = {w[1]:.2f}`
    - **Bias Inicial:** `b = {bias:.2f}`
    """)
    
    # Passo 1: Forward Pass
    if st.session_state.bp_step >= 1:
        with st.container(border=True):
            st.markdown("##### Passo 1: Forward Pass - Calcular a Predi√ß√£o da Rede")
            z = np.dot(w, x) + bias
            predicao = 1 / (1 + np.exp(-z)) # Fun√ß√£o Sigmoid

            st.latex(r"z = (w‚ÇÅ \times x‚ÇÅ) + (w‚ÇÇ \times x‚ÇÇ) + b")
            st.markdown(f"`z = ({w[0]:.2f} √ó {x[0]}) + ({w[1]:.2f} √ó {x[1]}) + {bias:.2f} = {z:.4f}`")

            st.latex(r"\text{predi√ß√£o (a)} = \sigma(z) = \frac{1}{1 + e^{-z}}")
            st.markdown(f"`predi√ß√£o = œÉ({z:.4f}) = {predicao:.4f}`")
            st.session_state.predicao = predicao
            st.session_state.z = z

    # Passo 2: Calcular o Erro
    if st.session_state.bp_step >= 2:
        with st.container(border=True):
            st.markdown("##### Passo 2: Calcular o Erro da Predi√ß√£o")
            erro = 0.5 * (y_real - st.session_state.predicao)**2 # Metade do Erro Quadr√°tico
            st.latex(r"E = \frac{1}{2} (y_{real} - \text{predi√ß√£o})^2")
            st.markdown(f"`E = 0.5 * ({y_real} - {st.session_state.predicao:.4f})¬≤ = {erro:.4f}`")

    # Passo 3: Backward Pass (C√°lculo dos Gradientes)
    if st.session_state.bp_step >= 3:
        with st.container(border=True):
            st.markdown("##### Passo 3: Backward Pass - Calcular os Gradientes (A M√°gica da Regra da Cadeia)")
            st.info("O objetivo √© descobrir a contribui√ß√£o de cada peso para o erro. Usamos a Regra da Cadeia do c√°lculo: `‚àÇE/‚àÇw = (‚àÇE/‚àÇpredi√ß√£o) * (‚àÇpredi√ß√£o/‚àÇz) * (‚àÇz/‚àÇw)`")

            # ‚àÇE/‚àÇpredi√ß√£o
            dE_dpred = st.session_state.predicao - y_real
            # ‚àÇpredi√ß√£o/‚àÇz (derivada da sigmoid)
            dpred_dz = st.session_state.predicao * (1 - st.session_state.predicao)
            # ‚àÇz/‚àÇw
            dz_dw1 = x[0]
            dz_dw2 = x[1]
            dz_dbias = 1.0

            st.markdown("**Componentes do Gradiente:**")
            st.markdown(f"- `‚àÇE/‚àÇpredi√ß√£o = predi√ß√£o - y_real = {dE_dpred:.4f}`")
            st.markdown(f"- `‚àÇpredi√ß√£o/‚àÇz = œÉ(z) * (1 - œÉ(z)) = {dpred_dz:.4f}`")
            st.markdown(f"- `‚àÇz/‚àÇw‚ÇÅ = x‚ÇÅ = {dz_dw1}`")

            st.session_state.dE_dpred = dE_dpred
            st.session_state.dpred_dz = dpred_dz

    # Passo 4: Juntar tudo para o gradiente final
    if st.session_state.bp_step >= 4:
        with st.container(border=True):
            st.markdown("##### Passo 4: Gradientes Finais")
            grad_w1 = st.session_state.dE_dpred * st.session_state.dpred_dz * x[0]
            grad_w2 = st.session_state.dE_dpred * st.session_state.dpred_dz * x[1]
            grad_bias = st.session_state.dE_dpred * st.session_state.dpred_dz * 1.0
            
            st.latex(r"\frac{\partial E}{\partial w_1} = ({dE_dpred:.2f}) \times ({st.session_state.dpred_dz:.2f}) \times ({x[0]}) = {grad_w1:.4f}")
            st.latex(r"\frac{\partial E}{\partial w_2} = ({dE_dpred:.2f}) \times ({st.session_state.dpred_dz:.2f}) \times ({x[1]}) = {grad_w2:.4f}")
            st.latex(r"\frac{\partial E}{\partial b} = ({dE_dpred:.2f}) \times ({st.session_state.dpred_dz:.2f}) \times (1) = {grad_bias:.4f}")

            # Salva para o pr√≥ximo passo
            st.session_state.grad_w1 = grad_w1
            st.session_state.grad_w2 = grad_w2
            st.session_state.grad_bias = grad_bias

    # Passo 5: Atualizar os pesos
    if st.session_state.bp_step >= 5:
        with st.container(border=True):
            st.markdown("##### Passo 5: Atualizar os Pesos - O Aprendizado Acontece!")
            st.success("Finalmente, ajustamos os pesos na dire√ß√£o OPOSTA ao gradiente, multiplicando pela taxa de aprendizado.")
            
            w_novo1 = w[0] - learning_rate * st.session_state.grad_w1
            w_novo2 = w[1] - learning_rate * st.session_state.grad_w2
            bias_novo = bias - learning_rate * st.session_state.grad_bias

            st.latex(r"w_{novo} = w_{antigo} - \eta \times \frac{\partial E}{\partial w}")
            
            st.markdown(f"**Novos Pesos:**")
            st.markdown(f"- `w‚ÇÅ_novo = {w[0]:.2f} - {learning_rate} √ó ({st.session_state.grad_w1:.4f}) = {w_novo1:.4f}`")
            st.markdown(f"- `w‚ÇÇ_novo = {w[1]:.2f} - {learning_rate} √ó ({st.session_state.grad_w2:.4f}) = {w_novo2:.4f}`")
            st.markdown(f"- `b_novo = {bias:.2f} - {learning_rate} √ó ({st.session_state.grad_bias:.4f}) = {bias_novo:.4f}`")
            
            st.markdown("\nEstes seriam os novos pesos para o pr√≥ximo ciclo de treinamento!")
            
    # Mensagem de fim
    if st.session_state.bp_step > 5:
        st.balloons()
        st.info("Voc√™ completou um passo de backpropagation! Clique em 'Reiniciar' para ver o processo novamente com diferentes taxas de aprendizado.")

elif secao == "üß† MLP em A√ß√£o":
    st.markdown('<h2 class="section-header">MLP em A√ß√£o: Resolvendo Problemas N√£o-Lineares</h2>', unsafe_allow_html=True)
    st.markdown("""
    Vamos ver o MLP resolver um problema que o Perceptron n√£o consegue. Para avaliar o modelo de forma realista, 
    vamos dividir nossos dados em um conjunto de **treino** (para ensinar o modelo) e um conjunto de **teste** 
    (para avaliar seu poder de generaliza√ß√£o com dados nunca vistos).
    """)
    
    col1, col2 = st.columns([1, 2])
    with col1:
        st.subheader("‚öôÔ∏è Par√¢metros do Modelo")
        hidden_layers = st.slider("Neur√¥nios na Camada Oculta", 2, 64, 10, key='mlp_h')
        activation = st.selectbox("Fun√ß√£o de Ativa√ß√£o", ["relu", "tanh", "logistic"], key='mlp_a')
        learning_rate_mlp = st.slider("Taxa de Aprendizado", 0.001, 0.1, 0.01, format="%.3f", key='mlp_lr')
        epochs_mlp = st.slider("M√°ximo de √âpocas", 100, 1000, 300, key='mlp_e')
        
        train_button = st.button("üöÄ Treinar e Avaliar MLP", type="primary")

    # 1. Gerar os dados
    X, y = make_circles(n_samples=300, noise=0.15, factor=0.5, random_state=1)
    X_scaled = StandardScaler().fit_transform(X)
    
    # 2. DIVIDIR OS DADOS EM TREINO E TESTE
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42
    )

    with col2:
        st.subheader("üìä Dados e Resultado")
        if train_button:
            model = MLPClassifier(
                hidden_layer_sizes=(hidden_layers,), 
                activation=activation, 
                solver='adam', 
                learning_rate_init=learning_rate_mlp, 
                max_iter=epochs_mlp, 
                random_state=1,
                early_stopping=True # Boa pr√°tica para evitar overfitting
            )
            
            # 3. TREINAR O MODELO APENAS COM OS DADOS DE TREINO
            model.fit(X_train, y_train)
            st.session_state.mlp_model = model
            
            # 4. AVALIAR EM AMBOS OS CONJUNTOS
            acc_train = model.score(X_train, y_train)
            acc_test = model.score(X_test, y_test)
            st.session_state.mlp_scores = (acc_train, acc_test)
            
            st.success("‚úÖ Modelo treinado e avaliado!")

        if 'mlp_model' in st.session_state:
            # Exibe as duas m√©tricas de acur√°cia
            acc_train, acc_test = st.session_state.mlp_scores
            st.metric("Acur√°cia no Treino (Dados Vistos)", f"{acc_train:.2%}")
            st.metric("Acur√°cia no Teste (Dados Novos - Generaliza√ß√£o)", f"{acc_test:.2%}")
            
            # A fronteira de decis√£o √© visualizada em todos os dados para um melhor entendimento
            boundary_fig = plot_decision_boundary(st.session_state.mlp_model, X_scaled, y)
            st.plotly_chart(boundary_fig, use_container_width=True)
            
            st.info("Observe como a acur√°cia no teste √© geralmente um pouco menor que no treino. Esta √© a medida mais honesta do desempenho do modelo!")
        else:
            # Antes de treinar, mostramos todos os dados para visualiza√ß√£o
            fig_data = px.scatter(x=X_scaled[:, 0], y=X_scaled[:, 1], color=y.astype(str), title="Dataset 'C√≠rculos' (N√£o-Linearmente Separ√°vel)")
            st.plotly_chart(fig_data, use_container_width=True)


elif secao == "üñºÔ∏è Redes Neurais Convolucionais (CNN)":
    st.markdown('<h2 class="section-header">Redes Neurais Convolucionais (CNNs)</h2>', unsafe_allow_html=True)
    # (Se√ß√£o CNN completa)
    st.markdown("CNNs s√£o uma classe de redes neurais especializada no processamento de dados com uma topologia de grade, como imagens.")
    with st.expander("üëÅÔ∏è A Opera√ß√£o de Convolu√ß√£o", expanded=True):
        st.markdown("A convolu√ß√£o aplica um **filtro (ou kernel)** sobre a imagem, criando um **mapa de caracter√≠sticas** que destaca padr√µes como bordas ou texturas.")
        col1, col2 = st.columns([1, 2])
        with col1:
            st.subheader("Filtros Interativos")
            image_data = np.zeros((10, 10)); image_data[2:8, 2:8] = 10
            kernels = {"Detector de Borda Vertical": np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]]), "Detector de Borda Horizontal": np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]), "Sharpen": np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])}
            kernel_choice = st.selectbox("Escolha um Kernel:", list(kernels.keys()))
            kernel = kernels[kernel_choice]
        with col2:
            convolved_image = convolve2d(image_data, kernel, mode='valid')
            fig, ax = plt.subplots(1, 3, figsize=(12, 4)); ax[0].imshow(image_data, cmap='gray'); ax[0].set_title('Imagem de Entrada'); ax[1].imshow(kernel, cmap='gray'); ax[1].set_title('Kernel'); ax[2].imshow(convolved_image, cmap='gray'); ax[2].set_title('Mapa de Caracter√≠sticas')
            for a in ax: a.axis('off')
            st.pyplot(fig)

elif secao == "üìú Redes Neurais Recorrentes (RNN)":
    st.markdown('<h2 class="section-header">Redes Neurais Recorrentes (RNNs)</h2>', unsafe_allow_html=True)
    # (Se√ß√£o RNN completa)
    st.markdown("RNNs s√£o projetadas para trabalhar com dados sequenciais. Sua caracter√≠stica definidora √© a **conex√£o recorrente**, que cria uma **mem√≥ria** para reter informa√ß√µes sobre o passado.")
    st.image("https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png", caption="Uma RNN 'desdobrada' no tempo. Fonte: Chris Olah's Blog.")
    with st.expander("üß† Solu√ß√£o para Mem√≥ria de Longo Prazo: LSTM e GRU", expanded=True):
        st.markdown("**Long Short-Term Memory (LSTM)** e **Gated Recurrent Unit (GRU)** usam 'port√µes' (gates) para regular o fluxo de informa√ß√£o, permitindo que a rede aprenda a reter ou descartar informa√ß√µes de forma seletiva.")
        st.image("https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png", caption="A estrutura de uma c√©lula LSTM com seus port√µes. Fonte: Chris Olah's Blog.")


elif secao == "üéÆ Playground Interativo":
    st.markdown('<h2 class="section-header">Playground Interativo de MLP</h2>', unsafe_allow_html=True)
    # (Se√ß√£o Playground completa e funcional)
    col1, col2, col3 = st.columns([1, 1, 2])
    with col1:
        st.subheader("üéØ Dados")
        dataset_type = st.selectbox("Tipo de Dataset", ["C√≠rculos", "Luas", "Linearmente Separ√°vel"])
        n_samples = st.slider("Amostras", 100, 1000, 300)
    with col2:
        st.subheader("üèóÔ∏è Arquitetura")
        hl_1 = st.slider("Neur√¥nios Camada 1", 1, 50, 10)
        hl_2 = st.slider("Neur√¥nios Camada 2", 0, 50, 5) # 0 para desativar
        activation_pg = st.selectbox("Ativa√ß√£o", ["relu", "tanh"], key='pg_act')
    if dataset_type == "Linearmente Separ√°vel": X, y = make_classification(n_samples=n_samples, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, random_state=42)
    elif dataset_type == "C√≠rculos": X, y = make_circles(n_samples=n_samples, noise=0.1, factor=0.5, random_state=42)
    else: X, y = make_moons(n_samples=n_samples, noise=0.15, random_state=42)
    X_scaled = StandardScaler().fit_transform(X)
    with col3:
        st.subheader("üìä Dados de Entrada")
        fig_data = px.scatter(x=X_scaled[:, 0], y=X_scaled[:, 1], color=y.astype(str)); fig_data.update_layout(showlegend=False, title=f"Dataset: {dataset_type}")
        st.plotly_chart(fig_data, use_container_width=True)
    if st.button("üöÄ TREINAR REDE NEURAL NO PLAYGROUND", type="primary"):
        hidden_layers_pg = (hl_1,) if hl_2 == 0 else (hl_1, hl_2)
        model_pg = MLPClassifier(hidden_layer_sizes=hidden_layers_pg, activation=activation_pg, solver='adam', max_iter=500, random_state=1, early_stopping=True, n_iter_no_change=20)
        with st.spinner("Treinando o modelo..."): model_pg.fit(X_scaled, y)
        st.session_state.pg_model = model_pg
        st.success("‚úÖ Treinamento Conclu√≠do!")
    if 'pg_model' in st.session_state:
        res_col1, res_col2 = st.columns(2)
        model = st.session_state.pg_model
        with res_col1:
            st.markdown("#### Fronteira de Decis√£o"); boundary_fig = plot_decision_boundary(model, X_scaled, y); st.plotly_chart(boundary_fig, use_container_width=True)
        with res_col2:
            st.markdown("#### M√©tricas"); st.metric("Acur√°cia no Treino", f"{model.score(X_scaled, y):.2%}"); st.metric("Itera√ß√µes", f"{model.n_iter_}"); st.metric("Perda Final", f"{model.loss_:.4f}")


# --- RODAP√â ---
st.markdown("---")
st.markdown("<div style='text-align: center; color: #666;'><strong>Guia Avan√ßado de Redes Neurais</strong> | Desenvolvido para o aprendizado aprofundado de conceitos de Machine Learning.</div>", unsafe_allow_html=True)